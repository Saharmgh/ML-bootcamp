{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AZq5YE3fr4oBCqldfGYHPXPDxtUxYbFC",
      "authorship_tag": "ABX9TyNJlLVEYbfEgoMwJBpWrN/X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saharmgh/ML-bootcamp/blob/main/DecisionTree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Algorithm from Scratch in Python\n",
        "\n",
        "Decision trees are machine learning models widely used for classification and regression tasks. I decided to build a decision tree from scratch to get a deeper understanding of the underlying principles behind this algorithm.\n",
        "\n",
        "In this Jupyter Notebook, we'll walk through the step-by-step process of creating a decision tree, and by the end of this tutorial, you'll have a functional decision tree implementation.\n",
        "\n",
        "\n",
        "## Outline\n",
        "1. **Understanding Decision Trees:** We'll start by exploring the intuition behind decision trees.\n",
        "\n",
        "2. **Dataset Preparation:** We'll use a simple dataset to illustrate the decision tree building process. This dataset will serve as the foundation for our implementation. Later you can test the algorithm with other datasets but keep in mind that feature values need to be categorical.\n",
        "\n",
        "3. **Building the Decision Tree:** We'll go step by step through the algorithm for constructing a decision tree, covering topics like entropy, and gini index.\n",
        "\n",
        "4. **Testing and Evaluation:** Once our decision tree is implemented, we'll test it on sample data and evaluate its performance.\n",
        "6. **Conclusion:** We'll wrap up the tutorial with a summary of what we've learned and discuss potential enhancements and extensions.\n",
        "\n",
        "Now, let's start."
      ],
      "metadata": {
        "id": "6cF9qy3VXdi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Understanding Decision Trees\n",
        "\n",
        "## What are Decision Trees?\n",
        "\n",
        "Decision tree is a supervised machine learning model used for both classification and regression tasks. Each decision corresponds to a node in the tree, leading to a conclusion at the leaves.\n",
        "\n",
        "## How Decision Trees Work\n",
        "\n",
        "At the root of the tree, the algorithm evaluates different features and selects the one that best separates the data based on some criterion. This process is repeated at each internal node, creating branches that represent different outcomes based on feature values. The leaves of the tree contain the final decisions or predictions.\n",
        "\n",
        "### Terminology:\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "[this article](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html).\n",
        "- **Root Node:** The topmost decision node in the tree.\n",
        "- **Internal Nodes:** Decision nodes that lead to further nodes.\n",
        "- **Leaves:** Terminal nodes that provide the final output (classification or regression).\n",
        "- **Splitting:** The process of dividing a node into sub-nodes based on a chosen criterion.\n",
        "- **Entropy, Information Gain and Gini Index:** Measures used to determine the best feature for splitting.\n",
        "\n",
        "### Intuition Behind Decision Trees\n",
        "\n",
        "Imagine you want to decide whether to play tennis on a given day. Your decision might depend on factors like the weather, temperature, and humidity. A decision tree captures this decision-making process by asking a series of questions like \"Is the weather sunny?\" or \"Is the temperature above 25 degrees?\" at each node until it reaches a leaf, providing a final decision."
      ],
      "metadata": {
        "id": "l8WdyfszXbU6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js40js47MNAX"
      },
      "outputs": [],
      "source": [
        "# import the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.lib.arraysetops import unique\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Dataset preparation"
      ],
      "metadata": {
        "id": "uQ8_7yQOllja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Here we work with Numpy dataset because it is relatively faster than Pandas for numerical operations."
      ],
      "metadata": {
        "id": "D8xQq4lquiAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Test with multiclass dataset:\n",
        "Definition_ A classification task where there are more than two classes, and the goal is to predict the correct class out of multiple possible classes."
      ],
      "metadata": {
        "id": "KKIZy2eMr7oD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imort the dataset and split it to test and train datasets\n",
        "# n_features = 5\n",
        "# X, y = make_blobs(centers=10, n_samples=1000, n_features=n_features)\n",
        "# y = pd.Series(y)\n",
        "# num_classes1= y.unique()\n",
        "# Xt, Xv, yt, yv = train_test_split(X, y, test_size=0.3)"
      ],
      "metadata": {
        "id": "nXJb3Z0J-tEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Test with binary classification dataset:\n",
        "Definition_ A classification task where the goal is to predict between two classes or categories."
      ],
      "metadata": {
        "id": "21pzzCTSsvnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imort the dataset and split it to test and train datasets\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data= load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "Xt, Xv, yt, yv =train_test_split(X, y, test_size= 0.3)"
      ],
      "metadata": {
        "id": "F4N6SBzhvZef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Decision Tree"
      ],
      "metadata": {
        "id": "qM8ezmZgZ0rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entropy in Decision Trees\n",
        "\n",
        "Entropy is a measure of disorder or impurity in a set of data. The goal of a decision tree algorithm is to minimize entropy, leading to well-defined and homogeneous subsets at each node of the tree.\n",
        "\n",
        "The entropy \\(H(S)\\) of a set \\(S\\) with respect to a binary classification (e.g., positive and negative classes) is calculated using the following formula:\n",
        "\n",
        "$ H(S) = -p_+ \\log_2(p_+) - p_- \\log_2(p_-) $\n",
        "\n",
        "Where:\n",
        "- $p_+$ is the proportion of positive examples in set \\(S\\).\n",
        "- $p_-$ is the proportion of negative examples in set \\(S\\).\n",
        "- $log_2$ is the base-2 logarithm.\n",
        "\n",
        "The entropy ranges from 0 to 1, where 0 indicates perfect homogeneity (pure set), and 1 indicates maximum impurity (equally distributed positive and negative examples).\n",
        "\n",
        "#### Interpretation\n",
        "\n",
        "- Low Entropy: A set with low entropy is more homogeneous and contains predominantly one class of examples. In decision tree terms, a low entropy implies a good feature for splitting the data.\n",
        "\n",
        "- High Entropy: A set with high entropy is more heterogeneous and contains a mix of different classes. Decision tree algorithms aim to find the features that minimize entropy, making the resulting subsets more pure.\n",
        "\n",
        "In the decision tree algorithm, the reduction in entropy, known as \"Information Gain,\" is used to evaluate the effectiveness of a feature in splitting the data. The feature that maximizes information gain is chosen as the splitting criterion at each internal node."
      ],
      "metadata": {
        "id": "dS1NLmMn4N2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(target):\n",
        "  p= target.value_counts()/ len(target)\n",
        "  h= np.sum(p*-np.log2(p))\n",
        "\n",
        "  return h"
      ],
      "metadata": {
        "id": "bbMGavRxOx0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gini Index in Decision Trees\n",
        "\n",
        "The Gini index is another measure of impurity commonly used in decision trees. Similar to entropy, the Gini index quantifies the uncertainty or disorder in a set of data.\n",
        "The Gini index \\(G(S)\\) of a set \\(S\\) with respect to a binary classification (e.g., positive and negative classes) is calculated using the following formula:\n",
        "\n",
        "$ G(S) = 1 - \\sum_{i=1}^{n} p_i^2 $\n",
        "\n",
        "Where:\n",
        "- $n$ is the number of classes.\n",
        "- $p_i$ is the proportion of examples in class $i$.\n",
        "\n",
        "The Gini index ranges from 0 to 1, where 0 indicates perfect homogeneity (pure set), and 1 indicates maximum impurity (equally distributed examples among classes).\n",
        "\n",
        "#### Interpretation\n",
        "\n",
        "- Low Gini Index: A set with a low Gini index is more homogeneous and contains predominantly one class of examples. In decision tree terms, a low Gini index implies a good feature for splitting the data.\n",
        "\n",
        "- High Gini Index: A set with a high Gini index is more heterogeneous and contains a mix of different classes. Decision tree algorithms aim to find the features that minimize the Gini index, making the resulting subsets more pure.\n",
        "\n",
        "The choice between using Gini index or entropy often depends on the specific application and the characteristics of the dataset."
      ],
      "metadata": {
        "id": "IXfBlj5C6HFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_index(target):\n",
        "  p= target.value_counts()/ len(target)\n",
        "  gini= 1- np.sum(p **2)\n",
        "  return gini"
      ],
      "metadata": {
        "id": "GHcBj3FNLLzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_index_calculation(target, clases, *groups):\n",
        "  class_size = []\n",
        "  group_size = []\n",
        "  split_size = 0\n",
        "\n",
        "  split_size = np.sum([len(g) for g in groups])\n",
        "\n",
        "  gini = 0\n",
        "  for gr in groups:\n",
        "    group_size = len(gr)\n",
        "\n",
        "    p = 0\n",
        "    for c in clases:\n",
        "      mask = target == c\n",
        "      class_size = len(gr.loc[mask])\n",
        "\n",
        "      if group_size == 0:\n",
        "        continue\n",
        "\n",
        "      p += (class_size / group_size)**2\n",
        "\n",
        "    g = np.sum((1 - p)) * (group_size / split_size)\n",
        "    gini += g\n",
        "\n",
        "  return gini"
      ],
      "metadata": {
        "id": "ijywQG8-_ZyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set splitting function for numerical features\n",
        "# TODO: how to split the categorical data.\n",
        "def set_split(data, feat, tresh):\n",
        "\n",
        "  sub_right = data[data[feat] < tresh]\n",
        "  sub_left = data[(data[feat] < tresh) ==False]\n",
        "  return sub_right, sub_left\n"
      ],
      "metadata": {
        "id": "0m0DSH1n09Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedy search to find the best split\n",
        "def best_split(data, target, feat, num_classes):\n",
        "  split_value= []\n",
        "  gini_index_value = []\n",
        "  gini_idx= []\n",
        "\n",
        "  best_split_results = []\n",
        "\n",
        "  for feature in feat:\n",
        "    gini_idx= []\n",
        "\n",
        "    i_ = data[feature].sort_values().unique()[1:]\n",
        "\n",
        "\n",
        "    for i in i_:\n",
        "\n",
        "      right_big, left_small = set_split(data, feature, i)\n",
        "      value_gini = gini_index_calculation(target, num_classes, right_big, left_small)\n",
        "\n",
        "      gini_idx.append(value_gini)\n",
        "    if len(gini_idx)==0:\n",
        "      return None,None,None\n",
        "    #get results with the lowest gini_index\n",
        "    best_gini_index_idx = np.argmin(gini_idx)\n",
        "    best_subset_gini = gini_idx[best_gini_index_idx]\n",
        "    best_subset_treshold= i_[best_gini_index_idx]\n",
        "\n",
        "    best_feature= feature\n",
        "    best_split_results.append((best_subset_gini, best_subset_treshold, best_feature))\n",
        "    best_split= min(best_split_results, key= lambda x:x[0])\n",
        "\n",
        "\n",
        "  return best_split"
      ],
      "metadata": {
        "id": "bUPEeUDxKRXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the ouput of this cell you will get something like this: The best split is for \"name of the feature\" when it is <= \"a specific treshold\". So, we can find the best candidate for the root node."
      ],
      "metadata": {
        "id": "M_ObZvRf4Q63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to build the tree?**"
      ],
      "metadata": {
        "id": "1Jb-q7X66WiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make the split\n",
        "def make_split(data, target, feat, num_classes):\n",
        "  best_values = best_split(data, target, feat, num_classes)\n",
        "  subsets= set_split(data, best_values[2], best_values[1])\n",
        "  return subsets"
      ],
      "metadata": {
        "id": "Soz-pkIP6tUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(label):\n",
        "  pred = label.value_counts().idxmax()\n",
        "  return pred"
      ],
      "metadata": {
        "id": "50zjuQBbYb1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the tree\n",
        "def train_tree(x, y, feat, max_depth=None, min_samples_split=None, max_gini=1, counter=0, max_categories=20):\n",
        "\n",
        "    if max_depth is None:\n",
        "        depth_cond = True\n",
        "    else:\n",
        "        depth_cond = counter < max_depth\n",
        "\n",
        "    if min_samples_split is None:\n",
        "        sample_cond = True\n",
        "    else:\n",
        "        sample_cond = x.shape[0] > min_samples_split\n",
        "\n",
        "\n",
        "    if depth_cond and sample_cond:\n",
        "        b_gini_value, b_tresh, b_feature = best_split(x, y , feat, num_classes1)\n",
        "\n",
        "        if b_gini_value is not None and b_gini_value <= max_gini:\n",
        "          # used_treshold.add(b_tresh)\n",
        "          counter += 1\n",
        "          left, right = set_split(x, b_feature, b_tresh)\n",
        "\n",
        "          split_type = \"<=\"\n",
        "          question = \"{} {} {}\".format(b_feature, split_type, b_tresh)\n",
        "          subtree = {question: []}\n",
        "\n",
        "          yes_answer = train_tree(left, y, feat,\n",
        "max_depth, min_samples_split, max_gini, counter)\n",
        "\n",
        "          no_answer = train_tree(right, y, feat,\n",
        "max_depth, min_samples_split, max_gini, counter)\n",
        "\n",
        "\n",
        "          if yes_answer == no_answer:\n",
        "            subtree =yes_answer\n",
        "          else:\n",
        "            subtree[question].append(yes_answer)\n",
        "            subtree[question].append(no_answer)\n",
        "\n",
        "          return subtree\n",
        "        else:\n",
        "          pred= make_prediction(y[x.index])\n",
        "          return pred\n",
        "    else:\n",
        "      # print(x.index)\n",
        "      pred= make_prediction(y[x.index])\n",
        "      return pred\n",
        "\n",
        "    return subtree"
      ],
      "metadata": {
        "id": "Gbg72k0Ej13c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classifier_dec(observations, tree):\n",
        "  question= list(tree.keys())[0]\n",
        "\n",
        "  if question.split()[1] == '<=':\n",
        "    if observations[question.split()[0]] <= float(question.split()[2]):\n",
        "      answer = tree[question][0]\n",
        "\n",
        "    else:\n",
        "      answer = tree[question][1]\n",
        "\n",
        "  if not isinstance(answer, dict):\n",
        "    return answer\n",
        "\n",
        "  else:\n",
        "    residual_tree = answer\n",
        "    return classifier_dec(observations, answer)"
      ],
      "metadata": {
        "id": "YFBZGVFBvnpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'predicted_labels' contains your predicted labels and 'actual_labels' contains the actual labels\n",
        "def calculate_accuracy(predicted_labels, actual_labels):\n",
        "    correct_predictions = sum(1 for p, a in zip(predicted_labels, actual_labels) if p == a)\n",
        "    total_predictions = len(predicted_labels)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "D1la8xKWBjjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining Functions into a Class\n",
        "\n",
        "In the previous sections, we've implemented various functions to construct a decision tree model. As our codebase grows, it becomes essential to organize these functions into a unified structure. This class will serve as a container for methods responsible for initializing the model, training the tree, making predictions, and any additional utility functions required for the decision tree implementation.\n",
        "\n",
        "Let's take a closer look at the class structure:"
      ],
      "metadata": {
        "id": "1hJ0a8sBWJwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTreemodel:\n",
        "\n",
        "    def __init__(self, max_depth= 7 , min_samples_split=20, max_gini=1, metric= 'gini'):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_gini = max_gini\n",
        "        self.tree = None\n",
        "        self.metric = metric\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _gini_index_calculation(target, *groups):\n",
        "        class_size = []\n",
        "        group_size = []\n",
        "        split_size = 0\n",
        "\n",
        "        split_size = np.sum([len(g) for g,y in groups])\n",
        "\n",
        "        gini = 0\n",
        "        for gr,y in groups:\n",
        "            group_size = len(gr)\n",
        "            p = 0\n",
        "            cl = np.unique(y)\n",
        "            for c in cl:\n",
        "                mask = y == c\n",
        "                class_size = np.sum(mask)\n",
        "\n",
        "                if group_size == 0:\n",
        "                    continue\n",
        "\n",
        "                p += (class_size / group_size) ** 2\n",
        "            assert p <= 1\n",
        "            g = np.sum((1 - p)) * (group_size / split_size)\n",
        "\n",
        "            gini += g\n",
        "\n",
        "        return gini\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _entropy_calculation(*groups):\n",
        "        class_size = []\n",
        "        group_size = []\n",
        "        split_size = 0\n",
        "\n",
        "        split_size = np.sum([len(g) for g,y in groups])\n",
        "        ent = 0\n",
        "        for gr,y in groups:\n",
        "            group_size = len(gr)\n",
        "            pe= 0\n",
        "            cl = np.unique(y)\n",
        "            eps = 1e-9\n",
        "            for c in cl:\n",
        "                mask = y == c\n",
        "                class_size = np.sum(mask)\n",
        "\n",
        "                if group_size == 0:\n",
        "                    continue\n",
        "\n",
        "                pe += (class_size / group_size) * np.log2((class_size + eps) / group_size)\n",
        "\n",
        "            assert pe <= 1\n",
        "            e = -np.sum(pe)\n",
        "            ent += e\n",
        "\n",
        "        return ent\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _set_split(data, target,j, tresh):\n",
        "\n",
        "        mask = data[:, j] < tresh\n",
        "        sub_right = data[mask]\n",
        "        y_right= target[mask]\n",
        "        sub_left = data[~mask]\n",
        "        y_left= target[~mask]\n",
        "\n",
        "        return sub_right, y_right, sub_left, y_left\n",
        "\n",
        "\n",
        "\n",
        "    def _best_split(self, data, target, num_classes):\n",
        "        assert isinstance(data, np.ndarray)\n",
        "        assert len(data.shape) == 2\n",
        "\n",
        "        split_value = []\n",
        "        gini_index_value = []\n",
        "        gini_idx = []\n",
        "        best_gini_split_results = []\n",
        "        best_ent_split_results = []\n",
        "\n",
        "        for j in range(data.shape[1]):\n",
        "            gini_idx = []\n",
        "            ent_idx = []\n",
        "            feature = data[:, j]\n",
        "            sorted_indices = np.argsort(feature)\n",
        "            i_ = np.unique(feature[sorted_indices])[1:]\n",
        "\n",
        "            for i in i_:\n",
        "                right_big, yr, left_small, yl = self._set_split(data, target, j, i)\n",
        "\n",
        "                if self.metric == 'gini':\n",
        "                    value_metric = self._gini_index_calculation(num_classes, (right_big, yr), (left_small, yl))\n",
        "                    gini_idx.append(value_metric)\n",
        "\n",
        "                    if len(gini_idx) == 0:\n",
        "                        return None, None, None\n",
        "                elif self.metric =='entropy':\n",
        "                    value_ent_metric = self._entropy_calculation((right_big, yr), (left_small, yl))\n",
        "                    ent_idx.append(value_ent_metric)\n",
        "                else:\n",
        "                   raise NotImplementedError\n",
        "\n",
        "\n",
        "\n",
        "            best_split_results = best_gini_split_results if self.metric == 'gini' else best_ent_split_results\n",
        "            best_metric_index_idx = np.argmin(gini_idx) if self.metric == 'gini' else np.argmin(ent_idx)\n",
        "            best_subset_metric = gini_idx[best_metric_index_idx] if self.metric == 'gini' else ent_idx[best_metric_index_idx]\n",
        "            best_subset_threshold = i_[best_metric_index_idx]\n",
        "\n",
        "            best_split_results.append((best_subset_metric, best_subset_threshold, j))\n",
        "\n",
        "        best_split_result = min(best_gini_split_results, key=lambda x: x[0]) if self.metric == 'gini' else min(best_ent_split_results, key=lambda x: x[0])\n",
        "\n",
        "        return best_split_result\n",
        "\n",
        "\n",
        "\n",
        "    def _make_prediction(self, label):\n",
        "\n",
        "      unique_values, counts = np.unique(label, return_counts=True)\n",
        "      pred = unique_values[np.argmax(counts)]\n",
        "\n",
        "      return pred\n",
        "\n",
        "    def train_tree(self, x, y, num_classes, counter=0):\n",
        "\n",
        "        if self.max_depth is None:\n",
        "            depth_cond = True\n",
        "        else:\n",
        "            depth_cond = counter < self.max_depth\n",
        "\n",
        "\n",
        "        if self.min_samples_split is None:\n",
        "            sample_cond = True\n",
        "        else:\n",
        "            sample_cond = x.shape[0] > self.min_samples_split\n",
        "\n",
        "\n",
        "        if depth_cond and sample_cond:\n",
        "            b_value, b_tresh, j = self._best_split(x, y, num_classes)\n",
        "            if b_value is not None and b_value <= self.max_gini:\n",
        "\n",
        "\n",
        "                counter += 1\n",
        "                right, y_right, left, y_left = self._set_split(x, y, j, b_tresh)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                split_type = \"<=\"\n",
        "                question = f\"{self.feat_to_idx[j]} {split_type} {b_tresh}\"\n",
        "                subtree = {question: []}\n",
        "\n",
        "                yes_answer = self.train_tree(right, y_right, num_classes, counter)\n",
        "                no_answer = self.train_tree(left, y_left, num_classes, counter)\n",
        "\n",
        "                if yes_answer == no_answer:\n",
        "                    subtree = yes_answer\n",
        "                else:\n",
        "                    subtree[question].append(yes_answer)\n",
        "                    subtree[question].append(no_answer)\n",
        "\n",
        "\n",
        "\n",
        "                return subtree\n",
        "            else:\n",
        "\n",
        "                pred = self._make_prediction(y)\n",
        "\n",
        "            return pred\n",
        "\n",
        "        else:\n",
        "\n",
        "          pred = self._make_prediction(y)\n",
        "\n",
        "          return pred\n",
        "\n",
        "        return subtree\n",
        "\n",
        "\n",
        "    def fit(self, data, target):\n",
        "\n",
        "      if isinstance(data, pd.DataFrame):\n",
        "        nu_class = target.unique()\n",
        "        self.feat_to_idx = {index : name for index, name in enumerate(data.columns)}\n",
        "        data = data.values\n",
        "      else:\n",
        "        nu_class= np.unique(target)\n",
        "        self.feat_to_idx = {i:i for i in range(data.shape[1])}\n",
        "\n",
        "      if isinstance(target, pd.DataFrame|pd.Series):\n",
        "        target = target.values\n",
        "      self.tree = self.train_tree(data, target, nu_class, counter=0)\n",
        "\n",
        "\n",
        "    def _classifier_dec(self, test, tree):\n",
        "      question= list(tree.keys())[0]\n",
        "      if question.split()[1] == '<=':\n",
        "        if test[int(question.split()[0])] <= float(question.split()[2]):\n",
        "          answer = tree[question][0]\n",
        "\n",
        "        else:\n",
        "          answer = tree[question][1]\n",
        "\n",
        "      if not isinstance(answer, dict):\n",
        "        return answer\n",
        "\n",
        "      else:\n",
        "        residual_tree = answer\n",
        "        return self._classifier_dec(test, answer)\n",
        "\n",
        "\n",
        "    def predict(self, test):\n",
        "        pred_label = []\n",
        "        tree_nodes= self.tree\n",
        "        for row in test:\n",
        "            results = self._classifier_dec(row, tree_nodes)\n",
        "            pred_label.append(results)\n",
        "        return pred_label\n",
        "model = DecisionTreemodel(metric= 'gini')\n",
        "#fit the model\n",
        "model.fit(Xt, yt)\n",
        "#make predictions\n",
        "yp = model.predict(Xv)\n",
        "print(f\"Acuuracy of implemented algorithm: {accuracy_score(yv, yp)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7Yc9JUEYPww",
        "outputId": "219cdddb-c66a-41fa-8cda-3c8a0d9c5428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acuuracy of implemented algorithm: 0.9298245614035088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecisionTreemodel(metric= 'entropy')\n",
        "#fit the model\n",
        "model.fit(Xt, yt)\n",
        "#make predictions\n",
        "yp = model.predict(Xv)\n",
        "print(f\"Acuuracy of implemented algorithm : {accuracy_score(yv, yp)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77P-ZSe9Dz1m",
        "outputId": "7f91d625-fb09-45a0-cab5-af544b9abba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acuuracy of implemented algorithm : 0.9005847953216374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the results with the sklearn decision tree\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn import tree\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf= clf.fit(Xt, yt)\n",
        "dec_pred= clf.predict(Xv)"
      ],
      "metadata": {
        "id": "RfhMEFfhkYf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dec_acc= accuracy_score(yv, dec_pred)\n",
        "print(f\"Acuuracy of built-in decision tree algorithm:{dec_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4AFrN8C3ZdH",
        "outputId": "656bbdfe-a5a6-4b2d-a50b-631f752db647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acuuracy of built-in decision tree algorithm:0.9298245614035088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1inEubIF0oqh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}